{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nwo_ai_challenge.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPE15SvSI8B9J4mhB7rpgW9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Pz6N8AAZEPc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f041202b-e687-428f-e1a0-a5d25ab1c9e0"
      },
      "source": [
        "from google.colab import auth\r\n",
        "auth.authenticate_user()\r\n",
        "print('Authenticated')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authenticated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY6AyzbUZxwM"
      },
      "source": [
        "### Challenge: Semantic Search Algorithm\r\n",
        "\r\n",
        "Design and implement a semantic search algorithm that is able to score and rank a\r\n",
        "set of keywords (trends) by how strongly associated they are to a given query term.\r\n",
        "The algorithmic approach could borrow techniques from association rule mining to\r\n",
        "analyze the co-occurrence of terms within a corpora of tweets and reddit posts, and\r\n",
        "should take into consideration the uniqueness of the trend and the recency of the\r\n",
        "association. For example, the algorithm should be able to determine that the query\r\n",
        "‘iPhone’ is more strongly associated to trends like ‘MagSafe’, ‘5G’, and ‘pacific blue'\r\n",
        "then it is to “Biden” or “perfume”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7BubXDsYMtg"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import nltk\r\n",
        "import spacy\r\n",
        "import string\r\n",
        "import re\r\n",
        "\r\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "\r\n",
        "stop_words = set(stopwords.words('english'))\r\n",
        "\r\n",
        "!python -m spacy download en_core_web_md\r\n",
        "import en_core_web_md\r\n",
        "nlp = en_core_web_md.load()\r\n",
        "\r\n",
        "!pip install transformers\r\n",
        "!pip install sentence_transformers\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "import transformers\r\n",
        "import scipy\r\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_pgLuCgZWv9"
      },
      "source": [
        "project_name = 'nwo-sample'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVrCShyYaCdh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79f3062b-fe58-424a-96a0-e94966c8293a"
      },
      "source": [
        "%time\r\n",
        "from google.cloud import bigquery\r\n",
        "\r\n",
        "client = bigquery.Client(project = project_name)\r\n",
        "\r\n",
        "# Perform a query.\r\n",
        "QUERY = ('SELECT * FROM `nwo-sample.graph.reddit` LIMIT 10000')\r\n",
        "\r\n",
        "query_job = client.query(QUERY)  # API request\r\n",
        "df = query_job.result().to_dataframe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 4.53 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W7jbE0dCR8Z"
      },
      "source": [
        "## Preprocess Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zCe-UC-CSMi"
      },
      "source": [
        "#Preprocess\r\n",
        "def remove_articles(text):\r\n",
        "  text = re.sub('\\s+(a|an|the|The|A|An)(\\s+)', ' ', text)\r\n",
        "  return text\r\n",
        "\r\n",
        "def remove_special_character(text):\r\n",
        "  text = re.sub('[^a-zA-Z0-9 ]', '', text)\r\n",
        "  return text\r\n",
        "\r\n",
        "#Get Noun words from string\r\n",
        "def noun_chunks(doc):\r\n",
        "  ret_list = []\r\n",
        "  for np in doc.noun_chunks:\r\n",
        "    if np.text not in ret_list:\r\n",
        "      ret_list.append(np.text)\r\n",
        "  return ret_list\r\n",
        "\r\n",
        "def get_nouns(text):\r\n",
        "  doc = nlp(text)\r\n",
        "  ret = noun_chunks(doc)\r\n",
        "  return ret\r\n",
        "\r\n",
        "#Replace the noun chunk's space with _\r\n",
        "def noun_chunk_space(doc):\r\n",
        "  noun_chunk = noun_chunks(doc)\r\n",
        "  for i in range(len(noun_chunk)):\r\n",
        "    noun_chunk[i] = re.sub('\\s', '_', noun_chunk[i])\r\n",
        "  return noun_chunk\r\n",
        "\r\n",
        "def replace_space(text):\r\n",
        "  doc = nlp(text)\r\n",
        "  noun_chunk = noun_chunks(doc)\r\n",
        "  replaced_nouns = noun_chunk_space(doc)\r\n",
        "  for i in range(len(noun_chunk)):\r\n",
        "    text = text.replace(noun_chunk[i], replaced_nouns[i])\r\n",
        "  return text\r\n",
        "\r\n",
        "def remove_punct(text):\r\n",
        "  return text.translate(str.maketrans('', '', string.punctuation))\r\n",
        "\r\n",
        "\r\n",
        "stop_words = set(stopwords.words('english'))\r\n",
        "\r\n",
        "def remove_stopwords(text):\r\n",
        "  text = ' '.join([word for word in text.split() if word not in stop_words])\r\n",
        "  return text\r\n",
        "\r\n",
        "def remove_url(text):\r\n",
        "  text = re.sub(r'http\\S+', '', text)\r\n",
        "  return text\r\n",
        "\r\n",
        "def preprocess(text):\r\n",
        "  text = remove_special_character(text)\r\n",
        "  text = remove_articles(text)\r\n",
        "  text = remove_punct(text)\r\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTva2TrEXjpj"
      },
      "source": [
        "## Trial 1\r\n",
        "\r\n",
        "Training Word2Vec with the texts, and find similar vocabs or noun_phrase in the text.\r\n",
        "\r\n",
        "In order to find the similarity between given query and words or context from the text data, I have trained gensim's Word2Vec model with tokenized words (concanate noun phrases with _). With the Word2Vec model, the given query will look up if the query is in trained corpus, if it does, the function will output top_n most similar vocabs or noun phrases. If it doesn't, the function will output error.\r\n",
        "\r\n",
        "Pro of this model is that it output not bad result if the query is existing in the model. However, as a con, the model cannot handle synonym or abbreviation if the vocab does not exist in the raw text data. This model can be improved with better Word Embedding model, Fast Text from facebook, where it takes word as n-gram not a token, hence can take unknown words as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAJlvKeP81Na"
      },
      "source": [
        "df['preprocess'] = df['body'].apply(preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IvmnyBv83G5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baae2a71-479a-49cc-d446-db3594d49e12"
      },
      "source": [
        "%%time\r\n",
        "\r\n",
        "df['preprocess'] = df['preprocess'].apply(replace_space)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 50s, sys: 253 ms, total: 1min 50s\n",
            "Wall time: 1min 50s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSc_Irv1BrMq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "27d9270e-e5c7-4acd-dc7e-6f88fc097b5f"
      },
      "source": [
        "df['preprocess'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Unfortunately most_dermatologists dont like or understand OCM It just doesnt make sense to them and theyre training and Ive found that its really hard to teach doctor new_tricks after theyve graduated and started working But science behind OCM is confusing and shouldnt make sense  but if it IS working for you then what_harm can it do httpsyoutubesK7UufZam2UBut I wouldnt ditch derm just because she didnt understand OCM Are you oily_Acne prone Using lot of prescriptions These are reasons its best to still be under care of dermatologist while still using OCM of finding new_products You can try to find another_derm though that you feel more comfortable with Secondly like doctors there are good_estheticians and bad_ones Do your_research Ask your_friends and family ask on your_local_subreddit or facebook_page look at genuine_reviews for salon Dont just pick fanciest_looking_salon and make appointment Sadly youll never really know until you drop money and go and if its bad then youll have to keep looking '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7obOEVi6hEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8399ebb3-ce93-48ed-fd9f-11e41d543ee2"
      },
      "source": [
        "%%time\r\n",
        "from gensim.models import Word2Vec\r\n",
        "\r\n",
        "training_data = []\r\n",
        "for sent in df['preprocess']:\r\n",
        "  training_data.append([word.lower() for word in sent.split()])\r\n",
        "\r\n",
        "#w2v model training\r\n",
        "w2v_model = Word2Vec(min_count=1,\r\n",
        "                     window=2,\r\n",
        "                     size=300,\r\n",
        "                     sample=6e-5, \r\n",
        "                     alpha=0.03, \r\n",
        "                     min_alpha=0.0007, \r\n",
        "                     negative=20)\r\n",
        "\r\n",
        "w2v_model.build_vocab(training_data, progress_per=10000)\r\n",
        "\r\n",
        "w2v_model.train(training_data, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 5s, sys: 576 ms, total: 3min 5s\n",
            "Wall time: 1min 10s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdkptOuVAZWU"
      },
      "source": [
        "def query_w2v(query, w2v_model, top_n = 10):\r\n",
        "  '''\r\n",
        "  Input: query: string, a word or phrase\r\n",
        "        w2v_model: gensim Word2Vec trained model\r\n",
        "\r\n",
        "  Output: top n most similar vocabs or phrase on trained Word2Vec\r\n",
        "  '''\r\n",
        "  #preprocess query\r\n",
        "  query = query.lower()\r\n",
        "  query = remove_special_character(query)\r\n",
        "  query = re.sub('\\s', '_', query)\r\n",
        "\r\n",
        "  #Retrieve similar words\r\n",
        "  similar_vocabs = w2v_model.wv.most_similar(positive = [query], topn = top_n)\r\n",
        "\r\n",
        "  return similar_vocabs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9MhsPB9ow4z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f041874b-5efe-4376-a160-1c262a17a1e5"
      },
      "source": [
        "query_w2v('Donald Trump', w2v_model, top_n = 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('usprsidenten', 0.8149840235710144),\n",
              " ('casually', 0.7828455567359924),\n",
              " ('fucking_idiot', 0.7643813490867615),\n",
              " ('this_mannerbecause', 0.7606171369552612),\n",
              " ('november_traf_sie_sich_mit_dem_designierten', 0.7519022226333618),\n",
              " ('und', 0.7278372645378113),\n",
              " ('solemn_pledge', 0.6685554385185242),\n",
              " ('ironically', 0.6450556516647339),\n",
              " ('boogeyman_tales', 0.6291605234146118),\n",
              " ('nationalist_uprising', 0.6145074963569641)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7QMTugImJvh"
      },
      "source": [
        "#Trial 2\r\n",
        "\r\n",
        "Using Sentence Bert to understand context and find similarity between given texts and query.\r\n",
        "\r\n",
        "I wanted to check whether the transfer learning helps for the semantic search. I believe it will definitely helps, but it does not help much on the below case. \r\n",
        "\r\n",
        "I have only took noun phrase from the sentences with spacy, then have embedded all with the pre-trained Sentence Bert model. With a given query, the function embeds the query and find most similar embedded noun_phrases by cosine similarity. However, as the noun_phrases are not clear enough, and sentence BERT has been pre-trained with sentences, not noun_phrases, it does not land a good result.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUHmi2rDi45Q"
      },
      "source": [
        "model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDkxUxhZhQ_E"
      },
      "source": [
        "def preprocess(text):\r\n",
        "  text = text.lower()\r\n",
        "  text = remove_stopwords(text)\r\n",
        "  text = remove_url(text)\r\n",
        "  text = remove_special_character(text)\r\n",
        "  text = remove_articles(text)\r\n",
        "  text = remove_punct(text)\r\n",
        "  \r\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h87TjJD-f5nM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ad2acd-fd72-4827-e2ee-92dcdd9aa95e"
      },
      "source": [
        "%%time\r\n",
        "\r\n",
        "df['preprocess'] = df['body'].apply(preprocess)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 191 ms, sys: 1.01 ms, total: 192 ms\n",
            "Wall time: 191 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mCq8mS2eydg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cf113f3-9f81-4a5c-f310-2567657be3e8"
      },
      "source": [
        "%%time\r\n",
        "\r\n",
        "def get_nouns(text):\r\n",
        "  doc = nlp(text)\r\n",
        "  ret = noun_chunks(doc)\r\n",
        "  return ret\r\n",
        "\r\n",
        "df['nouns'] = df['preprocess'].apply(get_nouns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 28s, sys: 284 ms, total: 1min 28s\n",
            "Wall time: 1min 28s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHIaoJ28TVev",
        "outputId": "b57d5db1-9171-4643-8646-4ddf80b86cd8"
      },
      "source": [
        "df['nouns'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['most dermatologists',\n",
              " 'ocm',\n",
              " 'sense',\n",
              " 'they',\n",
              " 'i',\n",
              " 'doctor',\n",
              " 'new tricks',\n",
              " 'science',\n",
              " 'working harm',\n",
              " 'derm',\n",
              " 'ocm oily acne',\n",
              " 'lot prescriptions reasons',\n",
              " 'dermatologist',\n",
              " 'ocm finding new products',\n",
              " 'another derm',\n",
              " 'doctors good estheticians',\n",
              " 'bad ones research',\n",
              " 'friends',\n",
              " 'family',\n",
              " 'local subreddit facebook page',\n",
              " 'genuine reviews',\n",
              " 'salon',\n",
              " 'fanciest looking salon',\n",
              " 'appointment',\n",
              " 'drop money']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUQ5o_4xet9v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ed5d2ac-68b7-4087-b2c6-61d10ac0ce0d"
      },
      "source": [
        "#only for noun_chuncks\r\n",
        "#store all nouns in list for embedding\r\n",
        "%%time\r\n",
        "\r\n",
        "d_word = {}\r\n",
        "for nouns in df['nouns']:\r\n",
        "  for noun in nouns:\r\n",
        "    if noun in d_word:\r\n",
        "      pass\r\n",
        "    else:\r\n",
        "      d_word[noun] = model.encode(noun)\r\n",
        "\r\n",
        "print(\"Total noun word & phrase in corpus: \", len(d_word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total noun word & phrase in corpus:  37312\n",
            "CPU times: user 8min 59s, sys: 27.2 s, total: 9min 27s\n",
            "Wall time: 9min 26s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vw0yvTinwyi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecc4238a-d6f8-4360-9e0a-fdfb6c27865f"
      },
      "source": [
        "%%time\r\n",
        "\r\n",
        "d_index = {}\r\n",
        "for key, value in enumerate(d_word):\r\n",
        "  d_index[key] = value\r\n",
        "  if key == 0:\r\n",
        "    arr = d_word[value]\r\n",
        "  else:\r\n",
        "    arr = np.vstack((arr, d_word[value]))\r\n",
        "\r\n",
        "print(arr.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(37312, 768)\n",
            "CPU times: user 9min 16s, sys: 14.8 s, total: 9min 31s\n",
            "Wall time: 9min 31s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q27GAa03rgxx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b4183d7-023c-44e9-c361-d330409c0c31"
      },
      "source": [
        "%%time\r\n",
        "from scipy.spatial import distance\r\n",
        "\r\n",
        "distances = distance.cdist([d_word[' book intelligent investor']], arr, 'cosine')[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 78.5 ms, sys: 22 ms, total: 100 ms\n",
            "Wall time: 100 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4FZo3Ljr1Iq"
      },
      "source": [
        "def get_top_n(query, d_index, arr, top_n = 10):\r\n",
        "  '''\r\n",
        "  Input: Query: string\r\n",
        "        d_index: dict, index of corresponding np array (embedded sentence)\r\n",
        "        top_n: int\r\n",
        "  OutPut: list: top n most similar embedded noun phrase related to query\r\n",
        "  '''\r\n",
        "\r\n",
        "  emb_query = model.encode(query)\r\n",
        "  distances = distance.cdist([emb_query], arr, 'cosine')[0]\r\n",
        "  top_n *= -1\r\n",
        "  top_n_ind = np.argpartition(distances, top_n)[top_n:]\r\n",
        "  ret = []\r\n",
        "  for ind in top_n_ind:\r\n",
        "    ret.append((d_index[ind], distances[ind]))\r\n",
        "  return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFbDbhX7s-vq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7281c9-4265-499c-938a-61292ab59d21"
      },
      "source": [
        "get_top_n('Donald Trump', d_index, arr, top_n = 20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('kids school etc cat shop', 0.9930142811111167),\n",
              " ('short conflict classmates friends', 0.9951881535942174),\n",
              " ('typical smallblockers', 0.9958957331807448),\n",
              " ('small variety foods', 0.9958199168314167),\n",
              " ('even small businesses', 0.995242731546684),\n",
              " ('poor innocent babies', 0.9958903279448739),\n",
              " ('weak jelly fish', 0.9973101241578203),\n",
              " ('physical abuse school nurse', 1.0032249980412395),\n",
              " ('existing small shop surroundings', 1.0039280293841895),\n",
              " ('talk school counselor depression anxiety counselor', 1.0042624749129578),\n",
              " ('really good hear kid school lunches', 1.0430745542856925),\n",
              " ('many small business owners', 1.0028514594397928),\n",
              " ('girls class', 1.031834770385056),\n",
              " ('rat crawl toilet', 0.9985858047626106),\n",
              " ('slowly sandblasted fine sand', 1.0160287910007435),\n",
              " ('cheapest whore whorehouse', 1.0029017439791874),\n",
              " ('kijiji kids violin', 1.007112975067471),\n",
              " ('feed kids lunch', 1.0164274705059746),\n",
              " ('poor girls', 1.0152680425011076),\n",
              " ('cafeteria lunch', 1.0014526682911895)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}